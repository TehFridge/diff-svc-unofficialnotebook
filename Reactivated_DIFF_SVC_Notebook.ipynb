{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP5rRkbTpnG8"
      },
      "source": [
        "# **[Diff-SVC](https://github.com/prophesier/diff-svc)**\n",
        "Singing Voice Conversion via diffusion model\n",
        "\n",
        "____\n",
        "\n",
        "####  **Notebook put together by [justinjohn-03](https://github.com/justinjohn0306)**\n",
        "\n",
        "## **Special thanks to [prophesier](https://github.com/prophesier) and [UtaUtaUtau](https://github.com/UtaUtaUtau)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mux_wwBggJKB",
        "outputId": "d9087df5-f8ca-49be-d4a2-3dc5749ea1df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#@title # Setup\n",
        "#@markdown ## Install Diff-SVC\n",
        "from IPython.display import clear_output \n",
        "from google.colab import files \n",
        "import gdown\n",
        "import os\n",
        "\n",
        "!rm -rf /content/sample_data\n",
        "\n",
        "\n",
        "Mode = \"install\" #@param [\"install\", \"update\", \"remove\"]\n",
        "Repository = \"Official Diff-SVC\" #@param [\"Official Diff-SVC\", \"UtaUtaUtau\"]\n",
        "Branch_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "repositories = {\n",
        "  'Official Diff-SVC':'prophesier',\n",
        "  'UtaUtaUtau':'UtaUtaUtau'\n",
        "}\n",
        "\n",
        "from pathlib import Path\n",
        "if Mode == 'install':\n",
        "  git_cmd = ''\n",
        "  if Branch_name: git_cmd += f\"-b {Branch_name} \"\n",
        "\n",
        "  git_cmd += f\"--depth 1 https://github.com/{repositories[Repository]}/diff-svc.git\"\n",
        "  !git clone $git_cmd\n",
        "  %cd /content/diff-svc\n",
        "  print('Installing torch')\n",
        "  !pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "  !pip install -r requirements_short.txt\n",
        "  !pip install tensorboard<2.9,>=2.8\n",
        "  %reload_ext tensorboard\n",
        "  print('Downloading pretrained models')\n",
        "  %cd \"/content/\"\n",
        "  %mkdir -p /content/diff-svc/checkpoints/\n",
        "  %cd \"/content/diff-svc/\"\n",
        "  !wget https://download1592.mediafire.com/f8p8qf1fp4cg/d1zcrvki20zc0bo/checkpoints.zip -O checkpoints.zip\n",
        "  !unzip /content/checkpoints.zip -d /content/diff-svc/\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "elif Mode == 'update':\n",
        "  %cd /content/diff-svc\n",
        "  !git pull\n",
        "  !pip install -r requirements_short.txt\n",
        "  clear_output()\n",
        "  print(\"Done!\")\n",
        "else:\n",
        "  answer = input(\"Are you sure you want to delete diff-svc folder? (y/n)\").lower()\n",
        "  while answer not in [\"y\", \"n\"]:\n",
        "    print(\"Invalid input\")\n",
        "    answer = input(\"Are you sure you want to delete diff-svc folder? (y/n)\").lower()\n",
        "  if answer == \"y\":\n",
        "    %cd /content\n",
        "    %rm -r diff-svc/\n",
        "    print(\"Done!\")\n",
        "  else:\n",
        "    print(\"Cancelled...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nnusihSv2a9"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Mount your Gdrive \n",
        "#@markdown (This is an essential step if you want to load your own trained model)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dlM9RgYviRhy"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "#@title # **Load model**\n",
        "\n",
        "#@markdown ### **Load the pretrained model (default)**\n",
        "\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown Note: Add the full path to the most recent checkpoint located on your Gdrive as well as the speaker's name if you wish to use your own model.\n",
        "\n",
        "#@markdown Example:-\n",
        "\n",
        "#@markdown The ``project_name`` will be the name of your speaker\n",
        "\n",
        "#@markdown  ``model_path: /content/drive/MyDrive/Diff-SVC/checkpoints/model_name/model_ckpt_steps_50000.ckpt``\n",
        "\n",
        "#@markdown           ``config_path: /content/drive/MyDrive/Diff-SVC/checkpoints/model_name/config.yaml``\n",
        "\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown ### **Set model location with the name of the speaker:**\n",
        "#@markdown *If you wish to use the pre-trained model and don't have your own model, leave these at their default values.*\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "%cd \"/content/diff-svc/\"\n",
        "\n",
        "os.environ['PYTHONPATH']='.'\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "\n",
        "from utils.hparams import hparams\n",
        "from preprocessing.data_gen_utils import get_pitch_parselmouth,get_pitch_crepe\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import utils\n",
        "import librosa\n",
        "import torchcrepe\n",
        "from infer import *\n",
        "import logging\n",
        "from infer_tools.infer_tool import *\n",
        "gdrive_id_model = \"\" #@param {type: \"string\"}\n",
        "gdrive_id_config= \"\" #@param {type: \"string\"}\n",
        "\n",
        "%cd \"/content/\"\n",
        "gdown.download(\n",
        "     \"https://drive.google.com/uc?export=download&confirm=pbef&id=\" + str(gdrive_id_model),\n",
        ")\n",
        "\n",
        "gdown.download(\n",
        "     \"https://drive.google.com/uc?export=download&confirm=pbef&id=\" + str(gdrive_id_config),\n",
        ")\n",
        "%cd \"/content/diff-svc/\"\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "\n",
        "# 工程文件夹名，训练时用的那个\n",
        "project_name = \"\" #@param {type: \"string\"}\n",
        "model_path = \"\" #@param {type: \"string\"}\n",
        "config_path=\"\" #@param {type: \"string\"}\n",
        "hubert_gpu=True\n",
        "svc_model = Svc(project_name,config_path,hubert_gpu, model_path)\n",
        "print('model loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mbmB1R2as9m2"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Upload your reference audio\n",
        "\n",
        "%cd \"/content/diff-svc/raw/\"\n",
        "\n",
        "print(\"\\n\\033[34m\\033[1mupload your reference audio\")\n",
        "listfn, length = files.upload().popitem()\n",
        "\n",
        "%cd \"/content/diff-svc/\"\n",
        "print(\"\\n\\033[32m\\033[1mdone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DauP3lXimfS3"
      },
      "outputs": [],
      "source": [
        "#@markdown # Input audio and adjust parameters\n",
        "\n",
        "%cd \"/content/diff-svc/\"\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``Path of input audio, default path is located in root directory``\n",
        "wav_fn='raw/1_videoplayback (8)_(Vocals).mp3' #@param {type: \"string\"}\n",
        "demoaudio, sr = librosa.load(wav_fn)\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``This shifts the raw audio up by one semitone before rendering, if the raw input is of a male voice and the desired voice is female, you can input 8 or 12 etc (12 would shift a whole octave).``\n",
        "key = 0#@param {type: \"integer\"}\n",
        "# 加速倍数\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``The multiple of the inference acceleration , the default value is 1000 steps, inputting a value if 10 would mean only using 100 steps to render, it's a rather straightforward value. The value can go up to 50x (rendering in 20 steps) without causing audible quality loss, if the value is set any higher it may start to cause quality loss.``\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "\n",
        "#@markdown #### Note: ``If use_gt_mel is set to True below, you should keep this value lower than the add_noise_step value and keep it at a value where it can completely divide 1000.``\n",
        "\n",
        "\n",
        "pndm_speedup = 20 #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``Path of input audio, default path is located in root directory``\n",
        "wav_gen='test_output.wav' #@param {type: \"string\"}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``Related to the use_gt_mel parameter, it controls the balance of the input and target voice, a value of 1 is completely the raw input, a value of 1000 is completely the target voice, there's an audible mix in tone when the value falls around 300 (this value isn't linear, also, if this parameter is set very low, you can decrease the pndm exceleration value for higher rendering quality)``\n",
        "add_noise_step = 1000 #@param {type: \"integer\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``Crepe's noise filter threshold, you can increase the value of the raw audio is clean, and if there is a lot of noise, you can keep or decrease the value, changing the use_crepe parameter to False will disable this parameter.``\n",
        "thre = 0.02 #@param {type: \"integer\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``Crepe is a F0 calculation algorithm, it's good but slow, setting the value to False will change the F0 calculation algorithm from crepe to parselmouth that is faster than crepe but is of lower quality``\n",
        "use_crepe= True #@param {type:\"boolean\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``F0 extraction algorithm for MEL spectogram rendering, using False will use the raw input's F0 for rendering. There's usually a difference in output between using True and False for rendering, usually setting it to True yields better results, but it's not set in stone, either value doesn't impact rendering speeds much. (Whatever the key value is, this is always changeable, doesn't affect it)``\n",
        "use_pe=True #@param {type:\"boolean\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``This option is similar to the image-to-image function of AI art generation, if set to True, the output audio shall be a mix of the input voice and the target voice, the percentage of each is decided by the next parameter.``\n",
        "\n",
        "#@markdown #### ``NOTE!!!: If this parameter is set to true, keep the key parameter value at 0, as rendering with various pitch input is not supported.``\n",
        "use_gt_mel= False #@param {type:\"boolean\"}\n",
        "#@markdown ___\n",
        "\n",
        "f0_tst, f0_pred, audio = run_clip(svc_model,file_path=wav_fn, key=key, acc=pndm_speedup, use_crepe=use_crepe, use_pe=use_pe, thre=thre,\n",
        "                                        use_gt_mel=use_gt_mel, add_noise_step=add_noise_step,project_name=project_name,out_path=wav_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ZQMM6ctnmizz"
      },
      "outputs": [],
      "source": [
        "#@markdown #Display results\n",
        "ipd.display(ipd.Audio(demoaudio, rate=sr))\n",
        "ipd.display(ipd.Audio(audio, rate=hparams['audio_sample_rate'], normalize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KduFEhQ8mmsM"
      },
      "outputs": [],
      "source": [
        "#@markdown #Display graph\n",
        "\n",
        "#f0_gen,_=get_pitch_crepe(*vocoder.wav2spec(wav_gen),hparams,threshold=0.05)\n",
        "%matplotlib inline\n",
        "f0_gen,_=get_pitch_parselmouth(*svc_model.vocoder.wav2spec(wav_gen),hparams)\n",
        "f0_tst[f0_tst==0]=np.nan#ground truth f0\n",
        "f0_pred[f0_pred==0]=np.nan#f0 pe predicted\n",
        "f0_gen[f0_gen==0]=np.nan#f0 generated\n",
        "fig=plt.figure(figsize=[15,5])\n",
        "plt.plot(np.arange(0,len(f0_tst)),f0_tst,color='black')\n",
        "plt.plot(np.arange(0,len(f0_pred)),f0_pred,color='orange')\n",
        "plt.plot(np.arange(0,len(f0_gen)),f0_gen,color='red')\n",
        "plt.axhline(librosa.note_to_hz('C4'),ls=\":\",c=\"blue\")\n",
        "plt.axhline(librosa.note_to_hz('G4'),ls=\":\",c=\"green\")\n",
        "plt.axhline(librosa.note_to_hz('C5'),ls=\":\",c=\"orange\")\n",
        "plt.axhline(librosa.note_to_hz('F#5'),ls=\":\",c=\"red\")\n",
        "#plt.axhline(librosa.note_to_hz('A#5'),ls=\":\",c=\"black\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}